{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**[Inspiração: Large Language Models are Effective Text Rankers with Pairwise Ranking\n",
        "Prompting](https://arxiv.org/pdf/2306.17563)**"
      ],
      "metadata": {
        "id": "YjXADvoxVYhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "Py3VqEehX1Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Sua Chave\""
      ],
      "metadata": {
        "id": "mMyVmcYkYUy6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# Definindo o modelo e a temperatura\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Template do prompt\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    HumanMessagePromptTemplate.from_template(\"\"\"\n",
        "        Given a query {query}, which of the following two passages is more relevant to the query?\"\n",
        "\n",
        "        Document A: {passage_a}\n",
        "\n",
        "        Document B: {passage_b}\n",
        "\n",
        "        Output: \"Document A\" or \"Document B\".\n",
        "    \"\"\")\n",
        "])\n",
        "\n",
        "# Dados de entrada\n",
        "query = \"What the best way to get clothes white\"\n",
        "\n",
        "passage_a = \"\"\"When in Doubt, Take a Cab. Taxis might be expensive in Puerto Rico,\n",
        "but they are safe and available. At night, it’s definitely the best way to get around. Look for the\n",
        "white taxis with the distinctive garita, or sentry box, icon painted on them.They are usually found\n",
        "at designated taxi stands.hen in Doubt, Take a Cab. Taxis might be expensive in Puerto Rico,\n",
        "but they are safe and available. At night, it’s definitely the best way to get around. Look for the\n",
        "white taxis with the distinctive garita, or sentry box, icon painted on them.\"\"\"\n",
        "\n",
        "passage_b = \"\"\"Thankfully, there are a couple of ways to prevent your whites from turning yellow: 1 Never bleach\n",
        "white clothing that is polyester or a polyester/cotton blend. 2 The chemical reaction between the\n",
        "bleach and the polyester almost always yields a yellowed result. 3 Consider a water softener if you\n",
        "have well-water.hankfully, there are a couple of ways to prevent your whites from turning yellow:\n",
        "1 Never bleach white clothing that is polyester or a polyester/cotton blend. 2 Consider a water\n",
        "softener if you have well-water. 3 Minimize your use of bleach altogether.\"\"\"\n",
        "\n",
        "# Preparando a chamada do modelo\n",
        "messages = chat_prompt.format_messages(query=query, passage_a=passage_a, passage_b=passage_b)\n",
        "\n",
        "# Executando o modelo\n",
        "response = chat(messages)\n",
        "\n",
        "# Resposta do modelo\n",
        "print(\"Resposta do modelo:\", response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMrERb0TZcyk",
        "outputId": "832de00e-9578-46a1-e44f-d1f48736e033"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta do modelo: Document B\n"
          ]
        }
      ]
    }
  ]
}